{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devices_dir = '/home/pravindran/mycode/tracolo/data/devices/'\n",
    "\n",
    "devices = {'FLMS00114': {'coeffs': np.loadtxt(os.path.join(devices_dir, 'FLMS00114_coeffs_raw.txt'), delimiter=';'),\n",
    "                         'upwell_fov_angle': 25, \n",
    "                         'dnwell_fov_angle': 25,\n",
    "                         'upwell_fiber_radius': 200, \n",
    "                         'dnwell_fiber_radius': 300,\n",
    "                         'upwell_factor': 1.0,\n",
    "                         'dnwell_factor': 2.0*math.pi}, \n",
    "           'NQ51B0141': {'coeffs': np.loadtxt(os.path.join(devices_dir, 'NQ51B0141_coeffs_raw.txt'), delimiter=';'),\n",
    "                         'upwell_fov_angle': 25, \n",
    "                         'dnwell_fov_angle': 25,\n",
    "                         'upwell_fiber_radius': 200, \n",
    "                         'dnwell_fiber_radius': 300,\n",
    "                         'upwell_factor': 1.0,\n",
    "                         'dnwell_factor': 2.0*math.pi}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to data directories\n",
    "# Emergence\n",
    "tractor_dir = '/home/pravindran/mycode/tracolo/data/emergence/Tractor/'\n",
    "piccolo_dir = '/home/pravindran/mycode/tracolo/data/emergence/Piccolo/'\n",
    "inter_dir = '/home/pravindran/mycode/tracolo/data/emergence/interpolated/'\n",
    "\n",
    "# Devices\n",
    "devices_dir = '/home/pravindran/mycode/tracolo/data/devices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_seconds(t):\n",
    "    '''\n",
    "    Convert HH:MM:SS to seconds since start of day.\n",
    "    '''\n",
    "    pcs = str(t).split(':')\n",
    "    h, m, s = float(pcs[0]), float(pcs[1]), float(pcs[2])\n",
    "    return (h*60.0 + m)*60.0 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tractor_dataframe(filepath, print_details=False):\n",
    "    '''\n",
    "    Reads tractor csv file.\n",
    "    Returns exp_id, date and pd.DataFrame.\n",
    "    Add date_stamp and time_stamp columns to pd.DataFrame.\n",
    "    '''\n",
    "    # Get the experiment id\n",
    "    tokens1 = os.path.split(filepath)\n",
    "    tokens2 = os.path.split(tokens1[0])\n",
    "    exp_id = tokens2[1]\n",
    "    \n",
    "    # Read the csv into pd.DataFrame\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Convert date time to number of seconds since start of day.\n",
    "    df['GPS_dt'] = pd.to_datetime(df['GPS_Date'] + \" \" + df['GPS_Time'])\n",
    "    df['date_stamp'] = df['GPS_dt'].dt.date\n",
    "    df['time_stamp'] = df['GPS_dt'].dt.time\n",
    "    date = str(df['date_stamp'].unique()[0])\n",
    "    df['secs_stamp'] = df['time_stamp'].apply(to_seconds)\n",
    "    # Sort by Point_ID to handle the unsorted time\n",
    "    df = df.sort_values('Point_ID', axis=0)\n",
    "    # Compute cluster ids for plot number runs.\n",
    "    # The tractor can revisit same plot more than once.\n",
    "    diffs = np.zeros(df.shape[0], dtype=np.int)\n",
    "    diffs[1:] = np.abs(np.diff(df['plot'].values))\n",
    "    diffs[0] = 1\n",
    "    diffs[diffs > 0] = 1\n",
    "    df['cluster_id'] = np.cumsum(diffs)\n",
    "    # Reset index for data frame\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    # Print some details\n",
    "    if print_details:\n",
    "        print(\"{}:{}\".format(exp_id, date))\n",
    "        print(\"\\tdf.shape: {}\".format(df.shape))\n",
    "        print(df['GPS_dt'].head())\n",
    "        print(df['date_stamp'].head())\n",
    "        print(df['time_stamp'].head())\n",
    "        print(\"--------------------------------\")\n",
    "    \n",
    "    return exp_id, date, df[['Point_ID', 'cluster_id', 'plot', \n",
    "                             'secs_stamp', 'Latitude', 'Longitude', \n",
    "                             'date_stamp', 'time_stamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_calibrated_spectrum_row(group, wave_cols, nonwave_cols, devices):\n",
    "    '''\n",
    "    Computes the calibrated spectrum. \n",
    "    Creates a new row from 4 rows.\n",
    "    '''\n",
    "    g = group[1]\n",
    "    d = devices[group[1]['device'].values[0]]\n",
    "    \n",
    "    # Normalize up\n",
    "    upwell_meas_row = g[(g['direction'] == 'Upwelling') & (g['dark'] == False)]\n",
    "    upwell_dark_row = g[(g['direction'] == 'Upwelling') & (g['dark'] == True)]\n",
    "    #### up parameters\n",
    "    upwell_area = math.pi*d['upwell_fiber_radius']*d['upwell_fiber_radius']\n",
    "    upwell_fov = d['upwell_fov_angle']*math.pi/180.0\n",
    "    upwell_factor = d['upwell_factor']\n",
    "    upwell_time = upwell_meas_row['integration_time'].values[0]\n",
    "    #### up dark corrected and normalized\n",
    "    upwell_dvsr = upwell_time*upwell_area*upwell_fov*upwell_factor\n",
    "    upwell_meas = np.array(upwell_meas_row[wave_cols].values, dtype=np.double)\n",
    "    upwell_dark = np.array(upwell_dark_row[wave_cols].values, dtype=np.double)\n",
    "    upwell_norm = (upwell_meas.flatten() - upwell_dark.flatten())/upwell_dvsr\n",
    "    \n",
    "    # Normalize dn\n",
    "    dnwell_meas_row = g[(g['direction'] == 'Downwelling') & (g['dark'] == False)]\n",
    "    dnwell_dark_row = g[(g['direction'] == 'Downwelling') & (g['dark'] == True)]\n",
    "    #### dn parameters\n",
    "    dnwell_area = math.pi*d['dnwell_fiber_radius']*d['dnwell_fiber_radius']\n",
    "    dnwell_fov = d['dnwell_fov_angle']*math.pi/180.0\n",
    "    dnwell_factor = d['dnwell_factor']\n",
    "    dnwell_time = dnwell_meas_row['integration_time'].values[0]\n",
    "    #### dn dark corrected and normalized\n",
    "    dnwell_dvsr = dnwell_time*dnwell_area*dnwell_fov*dnwell_factor\n",
    "    dnwell_meas = np.array(dnwell_meas_row[wave_cols].values, dtype=np.double)\n",
    "    dnwell_dark = np.array(dnwell_dark_row[wave_cols].values, dtype=np.double)\n",
    "    dnwell_norm = (dnwell_meas.flatten() - dnwell_dark.flatten())/dnwell_dvsr\n",
    "    \n",
    "    # Reflectance.\n",
    "    refls = list(upwell_norm*d['coeffs']/dnwell_norm)\n",
    "    \n",
    "    # Retain columns\n",
    "    others = list(upwell_meas_row[nonwave_cols].values[0])\n",
    "    \n",
    "    # Something weird is happening with the name of columns in in one of the csv files.\n",
    "    # So enforce that all wavelengths are float type\n",
    "    fwave_cols = [str(float(w)) for w in wave_cols]\n",
    "    \n",
    "    # set and return\n",
    "    return pd.DataFrame([others + refls], columns=(nonwave_cols + fwave_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_piccolo_dataframe(filepath, devices, print_details=False):\n",
    "    '''\n",
    "    Reads tractor csv file.\n",
    "    Returns exp_id, file id and pd.DataFrame.\n",
    "    Add date_stamp and time_stamp columns to pd.DataFrame.\n",
    "    '''\n",
    "    # Get the experiment id\n",
    "    tokens1 = os.path.split(filepath)\n",
    "    tokens2 = os.path.split(tokens1[0])\n",
    "    exp_id = tokens2[1]\n",
    "    \n",
    "    # Read the dataframe. Add GPS_Date and GPS_Time columns to it.\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "    df['GPS_dt'] = pd.to_datetime(df['datetime'])\n",
    "    df['date_stamp'] = df['GPS_dt'].dt.date\n",
    "    df['time_stamp'] = df['GPS_dt'].dt.time\n",
    "    df['secs_stamp'] = df['time_stamp'].apply(to_seconds)\n",
    "    df['secs_stamp'] -= 3600.0\n",
    "    date = str(df['date_stamp'].unique()[0])\n",
    "    \n",
    "\n",
    "    # Separate the columns into those that are wavelengths \n",
    "    # and those that are not wavelengths\n",
    "    nonwave_cols = []\n",
    "    wave_cols = []\n",
    "    for c in df.columns:\n",
    "        if is_numeric(str(c)):\n",
    "            wave_cols.append(str(c))\n",
    "        else:\n",
    "            nonwave_cols.append(str(c))\n",
    "            \n",
    "    # There will be 4 or 8 rows per filename.\n",
    "    # If 4, the device names should be the same.\n",
    "    # If 8, two devices are present with four rows per device.\n",
    "    grouped = df.groupby(['filename', 'device'], as_index=False)\n",
    "    rdfs = []\n",
    "    for g in grouped:\n",
    "        row = get_calibrated_spectrum_row(group=g,\n",
    "                                          wave_cols=wave_cols,\n",
    "                                          nonwave_cols=nonwave_cols,\n",
    "                                          devices=devices)\n",
    "        rdfs.append(row)\n",
    "    \n",
    "    rdf = pd.concat(rdfs)\n",
    "    rdf.reset_index()\n",
    "#     print(rdf.columns[10:20])\n",
    "    \n",
    "    # Print some details\n",
    "    if print_details:\n",
    "        print(\"{}:{}\".format(exp_id, date))\n",
    "        print(\"\\tdf.shape: {}\".format(df.shape))\n",
    "        print(df['GPS_dt'].head())\n",
    "        print(df['date_stamp'].head())\n",
    "        print(df['time_stamp'].head())\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "    return exp_id, date, rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys for tractor DataFrames are: \n",
      "1: exp16018:2016-06-17: (4056, 8)\n",
      "2: exp16018:2016-06-21: (4693, 8)\n",
      "3: exp16018:2016-06-27: (4424, 8)\n",
      "4: exp16018:2016-06-29: (4033, 8)\n",
      "5: exp16019:2016-06-17: (2060, 8)\n",
      "6: exp16019:2016-06-21: (4205, 8)\n",
      "7: exp16019:2016-06-27: (3924, 8)\n",
      "8: exp16019:2016-06-29: (4002, 8)\n",
      "9: exp16024:2016-07-25: (3755, 8)\n",
      "10: exp16024:2016-07-27: (4096, 8)\n",
      "11: exp16024:2016-08-01: (3801, 8)\n",
      "12: exp16024:2016-08-04: (3664, 8)\n",
      "13: exp16024:2016-08-08: (3602, 8)\n",
      "14: exp16025:2016-07-25: (4664, 8)\n",
      "15: exp16025:2016-07-27: (4211, 8)\n",
      "16: exp16025:2016-08-01: (4164, 8)\n",
      "17: exp16025:2016-08-04: (3646, 8)\n",
      "18: exp16025:2016-08-08: (4000, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load Tractor files\n",
    "tractor_subdirs = os.listdir(tractor_dir)\n",
    "tractor_subdirs.sort()\n",
    "tractor_subdirs = [os.path.join(tractor_dir, d) for d in tractor_subdirs]\n",
    "tractor_dfs = collections.OrderedDict()\n",
    "for sd in tractor_subdirs:\n",
    "    # Get the exp* sub-directories\n",
    "    filenames = os.listdir(sd)\n",
    "    filenames.sort()\n",
    "    # The files in the exp* directory are loaded.\n",
    "    # Saved in OrderedDict with key exp_id:file_id\n",
    "    for fn in filenames:\n",
    "        if fn[0] != '.':\n",
    "            exp_id, date, df = get_tractor_dataframe(os.path.join(sd, fn), print_details=False)\n",
    "            tractor_dfs[\":\".join([exp_id, date])] = df\n",
    "\n",
    "print(\"The keys for tractor DataFrames are: \")\n",
    "for i, k in enumerate(tractor_dfs):\n",
    "    print(\"{}: {}: {}\".format(i + 1, k, tractor_dfs[k].values.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N20160621_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160627_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160629_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_n20160617_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160617_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T20160621_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160627_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160629_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160725_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160727_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160801_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160804_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_N_20160808_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160725_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160727_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160801_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160804_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_T_20160808_17.csv\n",
      "nrows: 1333\n",
      "nrows: 1423\n",
      "nrows: 1480\n",
      "nrows: 1460\n",
      "nrows: 738\n",
      "nrows: 1484\n",
      "nrows: 1447\n",
      "nrows: 1460\n",
      "nrows: 1479\n",
      "nrows: 1531\n",
      "nrows: 1415\n",
      "nrows: 1412\n",
      "nrows: 1356\n",
      "nrows: 1411\n",
      "nrows: 1495\n",
      "nrows: 1439\n",
      "nrows: 1308\n",
      "nrows: 1360\n",
      "The keys for tractor DataFrames are: \n",
      "1: exp16018:2016-06-21: (1333, 694)\n",
      "2: exp16018:2016-06-27: (1423, 694)\n",
      "3: exp16018:2016-06-29: (1480, 694)\n",
      "4: exp16018:2016-06-17: (1460, 694)\n",
      "5: exp16019:2016-06-17: (738, 694)\n",
      "6: exp16019:2016-06-21: (1484, 694)\n",
      "7: exp16019:2016-06-27: (1447, 694)\n",
      "8: exp16019:2016-06-29: (1460, 694)\n",
      "9: exp16024:2016-07-25: (1479, 694)\n",
      "10: exp16024:2016-07-27: (1531, 694)\n",
      "11: exp16024:2016-08-01: (1415, 694)\n",
      "12: exp16024:2016-08-04: (1412, 694)\n",
      "13: exp16024:2016-08-08: (1356, 694)\n",
      "14: exp16025:2016-07-25: (1411, 694)\n",
      "15: exp16025:2016-07-27: (1495, 694)\n",
      "16: exp16025:2016-08-01: (1439, 694)\n",
      "17: exp16025:2016-08-04: (1308, 694)\n",
      "18: exp16025:2016-08-08: (1360, 694)\n"
     ]
    }
   ],
   "source": [
    "# Load Piccolo files\n",
    "piccolo_subdirs = os.listdir(piccolo_dir)\n",
    "piccolo_subdirs.sort()\n",
    "piccolo_subdirs = [os.path.join(piccolo_dir, d) for d in piccolo_subdirs]\n",
    "piccolo_piece_dfs = collections.OrderedDict()\n",
    "for sd in piccolo_subdirs:\n",
    "    # Get the exp* sub-directories\n",
    "    filenames = os.listdir(sd)\n",
    "    filenames.sort()\n",
    "    # The files in the exp* directory are loaded.\n",
    "    # Saved in OrderedDict with key exp_id:file_id\n",
    "    for fn in filenames:\n",
    "        print(fn)\n",
    "        exp_id, date, df = get_piccolo_dataframe(os.path.join(sd, fn), \n",
    "                                                 devices=devices, \n",
    "                                                 print_details=False)\n",
    "        key = \"{}:{}\".format(exp_id, date)\n",
    "        if key in piccolo_piece_dfs:\n",
    "            piccolo_piece_dfs[key].append(df)\n",
    "        else:\n",
    "            piccolo_piece_dfs[key] = [df]\n",
    "\n",
    "# Merge piece dfs.\n",
    "piccolo_dfs = collections.OrderedDict()\n",
    "for k in piccolo_piece_dfs:\n",
    "    piccolo_dfs[k] = pd.concat(piccolo_piece_dfs[k])\n",
    "    nrows = piccolo_dfs[k].shape[0]\n",
    "    print(\"nrows: {}\".format(nrows))\n",
    "    piccolo_dfs[k].index = np.arange(nrows)\n",
    "    \n",
    "# Print some stuff.    \n",
    "print(\"The keys for tractor DataFrames are: \")\n",
    "for i, k in enumerate(piccolo_dfs):\n",
    "    print(\"{}: {}: {}\".format(i + 1, k, piccolo_dfs[k].values.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Save the tractor dataframes into files\n",
    "# for (i, k) in enumerate(tractor_dfs):\n",
    "#     tractor_dfs[k].to_csv(os.path.join(inter_dir, k + '_tractor.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp16018:2016-06-21\n",
      "exp16018:2016-06-27\n",
      "exp16018:2016-06-29\n",
      "exp16018:2016-06-17\n",
      "exp16019:2016-06-17\n",
      "exp16019:2016-06-21\n",
      "exp16019:2016-06-27\n",
      "exp16019:2016-06-29\n",
      "exp16024:2016-07-25\n",
      "exp16024:2016-07-27\n",
      "exp16024:2016-08-01\n",
      "exp16024:2016-08-04\n",
      "exp16024:2016-08-08\n",
      "exp16025:2016-07-25\n",
      "exp16025:2016-07-27\n",
      "exp16025:2016-08-01\n",
      "exp16025:2016-08-04\n",
      "exp16025:2016-08-08\n"
     ]
    }
   ],
   "source": [
    "# Do the interpolation\n",
    "for k in piccolo_dfs:\n",
    "    print(k)\n",
    "    pdf = piccolo_dfs[k]\n",
    "    tdf = tractor_dfs[k]\n",
    "#     print(\"{}: min:{},   max: {}\".format(k, np.min(pdf['secs_stamp'].values), np.max(pdf['secs_stamp'].values)))\n",
    "#     print(\"{}: min:{},   max: {}\".format(k, np.min(tdf['secs_stamp'].values), np.max(tdf['secs_stamp'].values)))\n",
    "    pdf_lat = np.zeros(pdf.shape[0], dtype=np.double)\n",
    "    pdf_lon = np.zeros(pdf.shape[0], dtype=np.double)\n",
    "    pdf_plt = np.zeros(pdf.shape[0], dtype=np.int)\n",
    "    clusters = tractor_dfs[k].groupby('cluster_id')\n",
    "    for cluster in clusters:\n",
    "        if cluster[1]['secs_stamp'].values.size >= 4: \n",
    "#             print(cluster[1]['secs_stamp'].values.shape, cluster[1]['Latitude'].values.shape)\n",
    "            flat = interpolate.interp1d(cluster[1]['secs_stamp'].values, \n",
    "                                        cluster[1]['Latitude'].values, \n",
    "                                        bounds_error=False,\n",
    "                                        fill_value=0.0)\n",
    "            flon = interpolate.interp1d(cluster[1]['secs_stamp'].values, \n",
    "                                        cluster[1]['Longitude'].values, \n",
    "                                        bounds_error=False,\n",
    "                                        fill_value=0.0)\n",
    "            ilat = flat(pdf['secs_stamp'].values)\n",
    "            ilon = flon(pdf['secs_stamp'].values)\n",
    "            pdf_lat += ilat\n",
    "            pdf_lon += ilon\n",
    "            pdf_plt[np.where(np.abs(ilat) > 0.0)] = cluster[1]['plot'].unique()[0]\n",
    "    piccolo_dfs[k]['latitude'] = pdf_lat\n",
    "    piccolo_dfs[k]['longitude'] = pdf_lon\n",
    "    piccolo_dfs[k]['plot'] = pdf_plt\n",
    "    piccolo_dfs[k] = piccolo_dfs[k][piccolo_dfs[k]['plot'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the piccolo dataframes into files\n",
    "for (i, k) in enumerate(piccolo_dfs):\n",
    "    cols1 = ['filename', 'device', 'secs_stamp', 'plot', 'latitude', 'longitude']\n",
    "    cols2 = ['direction', 'dark', 'replicate', 'integration_time', 'datetime', \n",
    "             'GPS_dt', 'date_stamp', 'time_stamp']\n",
    "    cols = piccolo_dfs[k].columns.values\n",
    "    waves = []\n",
    "    for c in cols:\n",
    "        if c not in cols1 and c not in cols2:\n",
    "            waves.append(c)\n",
    "    piccolo_dfs[k].to_csv(os.path.join(inter_dir, k + '_piccolo.csv'),\n",
    "                          columns=(cols1 + waves + cols2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
