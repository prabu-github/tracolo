{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devices_dir = '/home/pravindran/mycode/tracolo/data/devices/'\n",
    "\n",
    "devices = {'FLMS00114': {'coeffs': np.loadtxt(os.path.join(devices_dir, 'FLMS00114_coeffs_raw.txt'), delimiter=';'),\n",
    "                         'upwell_fov_angle': 25, \n",
    "                         'dnwell_fov_angle': 25,\n",
    "                         'upwell_fiber_radius': 200, \n",
    "                         'dnwell_fiber_radius': 300,\n",
    "                         'upwell_factor': 1.0,\n",
    "                         'dnwell_factor': 2.0*math.pi}, \n",
    "           'NQ51B0141': {'coeffs': np.loadtxt(os.path.join(devices_dir, 'NQ51B0141_coeffs_raw.txt'), delimiter=';'),\n",
    "                         'upwell_fov_angle': 25, \n",
    "                         'dnwell_fov_angle': 25,\n",
    "                         'upwell_fiber_radius': 200, \n",
    "                         'dnwell_fiber_radius': 300,\n",
    "                         'upwell_factor': 1.0,\n",
    "                         'dnwell_factor': 2.0*math.pi}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to data directories\n",
    "# sds\n",
    "tractor_dir = '/home/pravindran/mycode/tracolo/data/sds/Tractor/'\n",
    "piccolo_dir = '/home/pravindran/mycode/tracolo/data/sds/Piccolo/'\n",
    "inter_dir = '/home/pravindran/mycode/tracolo/data/sds/interpolated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_seconds(t):\n",
    "    '''\n",
    "    Convert HH:MM:SS to seconds since start of day.\n",
    "    '''\n",
    "    pcs = str(t).split(':')\n",
    "    h, m, s = float(pcs[0]), float(pcs[1]), float(pcs[2])\n",
    "    return (h*60.0 + m)*60.0 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: emergence-->sds::plot-->Plot\n",
    "def get_tractor_dataframe(filepath, print_details=False):\n",
    "    '''\n",
    "    Reads tractor csv file.\n",
    "    Returns exp_id, date and pd.DataFrame.\n",
    "    Add date_stamp and time_stamp columns to pd.DataFrame.\n",
    "    '''\n",
    "    # Get the experiment id\n",
    "    tokens1 = os.path.split(filepath)\n",
    "    tokens2 = os.path.split(tokens1[0])\n",
    "    exp_id = tokens2[1]\n",
    "    \n",
    "    # Read the csv into pd.DataFrame\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Convert date time to number of seconds since start of day.\n",
    "    df['GPS_dt'] = pd.to_datetime(df['GPS_Date'] + \" \" + df['GPS_Time'])\n",
    "    df['date_stamp'] = df['GPS_dt'].dt.date\n",
    "    df['time_stamp'] = df['GPS_dt'].dt.time\n",
    "    date = str(df['date_stamp'].unique()[0])\n",
    "    df['secs_stamp'] = df['time_stamp'].apply(to_seconds)\n",
    "    # Sort by Point_ID to handle the unsorted time\n",
    "    df = df.sort_values('Point_ID', axis=0)\n",
    "    # Compute cluster ids for plot number runs.\n",
    "    # The tractor can revisit same plot more than once.\n",
    "    diffs = np.zeros(df.shape[0], dtype=np.int)\n",
    "    diffs[1:] = np.abs(np.diff(df['Plot'].values))\n",
    "    diffs[0] = 1\n",
    "    diffs[diffs > 0] = 1\n",
    "    df['cluster_id'] = np.cumsum(diffs)\n",
    "    # Reset index for data frame\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    # Print some details\n",
    "    if print_details:\n",
    "        print(\"{}:{}\".format(exp_id, date))\n",
    "        print(\"\\tdf.shape: {}\".format(df.shape))\n",
    "        print(df['GPS_dt'].head())\n",
    "        print(df['date_stamp'].head())\n",
    "        print(df['time_stamp'].head())\n",
    "        print(\"--------------------------------\")\n",
    "    \n",
    "    \n",
    "    return exp_id, date, df[['Point_ID', 'cluster_id', 'Plot', \n",
    "                             'secs_stamp', 'Latitude', 'Longitude', \n",
    "                             'date_stamp', 'time_stamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_calibrated_spectrum_row(group, wave_cols, nonwave_cols, devices):\n",
    "    '''\n",
    "    Computes the calibrated spectrum. \n",
    "    Creates a new row from 4 rows.\n",
    "    '''\n",
    "    g = group[1]\n",
    "    d = devices[group[1]['device'].values[0]]\n",
    "    \n",
    "    # Normalize up\n",
    "    upwell_meas_row = g[(g['direction'] == 'Upwelling') & (g['dark'] == False)]\n",
    "    upwell_dark_row = g[(g['direction'] == 'Upwelling') & (g['dark'] == True)]\n",
    "    #### up parameters\n",
    "    upwell_area = math.pi*d['upwell_fiber_radius']*d['upwell_fiber_radius']\n",
    "    upwell_fov = d['upwell_fov_angle']*math.pi/180.0\n",
    "    upwell_factor = d['upwell_factor']\n",
    "    upwell_time = upwell_meas_row['integration_time'].values[0]\n",
    "    #### up dark corrected and normalized\n",
    "    upwell_dvsr = upwell_time*upwell_area*upwell_fov*upwell_factor\n",
    "    upwell_meas = np.array(upwell_meas_row[wave_cols].values, dtype=np.double)\n",
    "    upwell_dark = np.array(upwell_dark_row[wave_cols].values, dtype=np.double)\n",
    "    upwell_norm = (upwell_meas.flatten() - upwell_dark.flatten())/upwell_dvsr\n",
    "    \n",
    "    # Normalize dn\n",
    "    dnwell_meas_row = g[(g['direction'] == 'Downwelling') & (g['dark'] == False)]\n",
    "    dnwell_dark_row = g[(g['direction'] == 'Downwelling') & (g['dark'] == True)]\n",
    "    #### dn parameters\n",
    "    dnwell_area = math.pi*d['dnwell_fiber_radius']*d['dnwell_fiber_radius']\n",
    "    dnwell_fov = d['dnwell_fov_angle']*math.pi/180.0\n",
    "    dnwell_factor = d['dnwell_factor']\n",
    "    dnwell_time = dnwell_meas_row['integration_time'].values[0]\n",
    "    #### dn dark corrected and normalized\n",
    "    dnwell_dvsr = dnwell_time*dnwell_area*dnwell_fov*dnwell_factor\n",
    "    dnwell_meas = np.array(dnwell_meas_row[wave_cols].values, dtype=np.double)\n",
    "    dnwell_dark = np.array(dnwell_dark_row[wave_cols].values, dtype=np.double)\n",
    "    dnwell_norm = (dnwell_meas.flatten() - dnwell_dark.flatten())/dnwell_dvsr\n",
    "    \n",
    "    # NOTE: Modifications for SDS.\n",
    "    # Reflectance.\n",
    "    not_nan_idxs = np.logical_not(np.isnan(upwell_norm))\n",
    "    not_nan_refls = list(upwell_norm[not_nan_idxs]*d['coeffs']/dnwell_norm[not_nan_idxs])\n",
    "    refls = np.zeros(upwell_norm.shape, dtype=upwell_norm.dtype)\n",
    "    refls.fill(np.nan)\n",
    "    refls[not_nan_idxs] = not_nan_refls\n",
    "    refls = list(refls)\n",
    "    \n",
    "    # Retain columns\n",
    "    others = list(upwell_meas_row[nonwave_cols].values[0])\n",
    "    \n",
    "    # Something weird is happening with the name of columns in in one of the csv files.\n",
    "    # So enforce that all wavelengths are float type\n",
    "    fwave_cols = [str(float(w)) for w in wave_cols]\n",
    "    \n",
    "    # set and return\n",
    "    return pd.DataFrame([others + refls], columns=(nonwave_cols + fwave_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_piccolo_dataframe(filepath, devices, print_details=False):\n",
    "    '''\n",
    "    Reads tractor csv file.\n",
    "    Returns exp_id, file id and pd.DataFrame.\n",
    "    Add date_stamp and time_stamp columns to pd.DataFrame.\n",
    "    '''\n",
    "    # Get the experiment id\n",
    "    tokens1 = os.path.split(filepath)\n",
    "    tokens2 = os.path.split(tokens1[0])\n",
    "    exp_id = tokens2[1]\n",
    "    \n",
    "    # Read the dataframe. Add GPS_Date and GPS_Time columns to it.\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "    df['GPS_dt'] = pd.to_datetime(df['datetime'])\n",
    "    df['date_stamp'] = df['GPS_dt'].dt.date\n",
    "    df['time_stamp'] = df['GPS_dt'].dt.time\n",
    "    df['secs_stamp'] = df['time_stamp'].apply(to_seconds)\n",
    "    df['secs_stamp'] -= 3600.0\n",
    "    date = str(df['date_stamp'].unique()[0])\n",
    "    \n",
    "\n",
    "    # Separate the columns into those that are wavelengths \n",
    "    # and those that are not wavelengths\n",
    "    nonwave_cols = []\n",
    "    wave_cols = []\n",
    "    for c in df.columns:\n",
    "        if is_numeric(str(c)):\n",
    "            wave_cols.append(str(c))\n",
    "        else:\n",
    "            nonwave_cols.append(str(c))\n",
    "            \n",
    "    # There will be 4 or 8 rows per filename.\n",
    "    # If 4, the device names should be the same.\n",
    "    # If 8, two devices are present with four rows per device.\n",
    "    grouped = df.groupby(['filename', 'device'], as_index=False)\n",
    "    rdfs = []\n",
    "    for g in grouped:\n",
    "        row = get_calibrated_spectrum_row(group=g,\n",
    "                                          wave_cols=wave_cols,\n",
    "                                          nonwave_cols=nonwave_cols,\n",
    "                                          devices=devices)\n",
    "        rdfs.append(row)\n",
    "    \n",
    "    rdf = pd.concat(rdfs)\n",
    "    rdf.reset_index()\n",
    "#     print(rdf.columns[10:20])\n",
    "    \n",
    "    # Print some details\n",
    "    if print_details:\n",
    "        print(\"{}:{}\".format(exp_id, date))\n",
    "        print(\"\\tdf.shape: {}\".format(df.shape))\n",
    "        print(df['GPS_dt'].head())\n",
    "        print(df['date_stamp'].head())\n",
    "        print(df['time_stamp'].head())\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "    return exp_id, date, rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys for tractor DataFrames are: \n",
      "1: exp16045:2016-06-08: (4107, 8)\n",
      "2: exp16045:2016-06-28: (5095, 8)\n",
      "3: exp16045:2016-07-18: (4956, 8)\n",
      "4: exp16045:2016-07-26: (5231, 8)\n",
      "5: exp16045:2016-08-15: (5030, 8)\n",
      "6: exp16045:2016-08-22: (4922, 8)\n",
      "7: exp16045:2016-09-02: (4779, 8)\n",
      "8: exp16045:2016-09-12: (2503, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load Tractor files\n",
    "tractor_subdirs = os.listdir(tractor_dir)\n",
    "tractor_subdirs.sort()\n",
    "tractor_subdirs = [os.path.join(tractor_dir, d) for d in tractor_subdirs]\n",
    "tractor_dfs = collections.OrderedDict()\n",
    "for sd in tractor_subdirs:\n",
    "    # Get the exp* sub-directories\n",
    "    filenames = os.listdir(sd)\n",
    "    filenames.sort()\n",
    "    # The files in the exp* directory are loaded.\n",
    "    # Saved in OrderedDict with key exp_id:file_id\n",
    "    for fn in filenames:\n",
    "        if fn[0] != '.':\n",
    "            exp_id, date, df = get_tractor_dataframe(os.path.join(sd, fn), print_details=False)\n",
    "            tractor_dfs[\":\".join([exp_id, date])] = df\n",
    "\n",
    "print(\"The keys for tractor DataFrames are: \")\n",
    "for i, k in enumerate(tractor_dfs):\n",
    "    print(\"{}: {}: {}\".format(i + 1, k, tractor_dfs[k].values.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160608_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160608_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160608_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_20.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_21.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_22.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_23.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_24.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160628_25.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_20.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_21.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_22.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_24.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_25.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_27.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160718_28.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160726_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160815_20.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160822_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_01.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_05.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_06.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_07.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_08.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_10.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_16.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_17.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160902_19.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_02.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_03.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_04.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_09.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_11.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_12.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_13.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_14.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_15.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_18.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_20.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_21.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_22.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_23.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_24.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_25.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_26.csv\n",
      "C___Users_iherrmann.RUSSELL_Desktop_temp_S_20160912_27.csv\n",
      "nrows: 2130\n",
      "nrows: 2154\n",
      "nrows: 2158\n",
      "nrows: 1874\n",
      "nrows: 1848\n",
      "nrows: 1780\n",
      "nrows: 1786\n",
      "nrows: 516\n",
      "The keys for piccolo DataFrames are: \n",
      "1: exp16045:2016-06-08: (2130, 1385)\n",
      "2: exp16045:2016-06-28: (2154, 1385)\n",
      "3: exp16045:2016-07-18: (2158, 1385)\n",
      "4: exp16045:2016-07-26: (1874, 1385)\n",
      "5: exp16045:2016-08-15: (1848, 1385)\n",
      "6: exp16045:2016-08-22: (1780, 1385)\n",
      "7: exp16045:2016-09-02: (1786, 1385)\n",
      "8: exp16045:2016-09-12: (516, 1385)\n"
     ]
    }
   ],
   "source": [
    "# Load Piccolo files\n",
    "piccolo_subdirs = os.listdir(piccolo_dir)\n",
    "piccolo_subdirs.sort()\n",
    "piccolo_subdirs = [os.path.join(piccolo_dir, d) for d in piccolo_subdirs]\n",
    "piccolo_piece_dfs = collections.OrderedDict()\n",
    "for sd in piccolo_subdirs:\n",
    "    # Get the exp* sub-directories\n",
    "    filenames = os.listdir(sd)\n",
    "    filenames.sort()\n",
    "    # The files in the exp* directory are loaded.\n",
    "    # Saved in OrderedDict with key exp_id:file_id\n",
    "    for fn in filenames:\n",
    "        print(fn)\n",
    "        exp_id, date, df = get_piccolo_dataframe(os.path.join(sd, fn), \n",
    "                                                 devices=devices, \n",
    "                                                 print_details=False)\n",
    "        key = \"{}:{}\".format(exp_id, date)\n",
    "        if key in piccolo_piece_dfs:\n",
    "            piccolo_piece_dfs[key].append(df)\n",
    "        else:\n",
    "            piccolo_piece_dfs[key] = [df]\n",
    "\n",
    "# Merge piece dfs.\n",
    "piccolo_dfs = collections.OrderedDict()\n",
    "for k in piccolo_piece_dfs:\n",
    "    piccolo_dfs[k] = pd.concat(piccolo_piece_dfs[k])\n",
    "    nrows = piccolo_dfs[k].shape[0]\n",
    "    print(\"nrows: {}\".format(nrows))\n",
    "    piccolo_dfs[k].index = np.arange(nrows)\n",
    "    \n",
    "# Print some stuff.    \n",
    "print(\"The keys for piccolo DataFrames are: \")\n",
    "for i, k in enumerate(piccolo_dfs):\n",
    "    print(\"{}: {}: {}\".format(i + 1, k, piccolo_dfs[k].values.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp16045:2016-06-08\n",
      "exp16045:2016-06-28\n",
      "exp16045:2016-07-18\n",
      "exp16045:2016-07-26\n",
      "exp16045:2016-08-15\n",
      "exp16045:2016-08-22\n",
      "exp16045:2016-09-02\n",
      "exp16045:2016-09-12\n"
     ]
    }
   ],
   "source": [
    "# Do the interpolation\n",
    "for k in piccolo_dfs:\n",
    "    print(k)\n",
    "    if k in tractor_dfs:\n",
    "        pdf = piccolo_dfs[k]\n",
    "        tdf = tractor_dfs[k]\n",
    "        # print(\"{}: min:{},   max: {}\".format(k, np.min(pdf['secs_stamp'].values), np.max(pdf['secs_stamp'].values)))\n",
    "        # print(\"{}: min:{},   max: {}\".format(k, np.min(tdf['secs_stamp'].values), np.max(tdf['secs_stamp'].values)))\n",
    "        pdf_lat = np.zeros(pdf.shape[0], dtype=np.double)\n",
    "        pdf_lon = np.zeros(pdf.shape[0], dtype=np.double)\n",
    "        pdf_plt = np.zeros(pdf.shape[0], dtype=np.int)\n",
    "        clusters = tractor_dfs[k].groupby('cluster_id')\n",
    "        for cluster in clusters:\n",
    "            if cluster[1]['secs_stamp'].values.size >= 4: \n",
    "                #             print(cluster[1]['secs_stamp'].values.shape, cluster[1]['Latitude'].values.shape)\n",
    "                flat = interpolate.interp1d(cluster[1]['secs_stamp'].values, \n",
    "                                            cluster[1]['Latitude'].values, \n",
    "                                            bounds_error=False,\n",
    "                                            fill_value=0.0)\n",
    "                flon = interpolate.interp1d(cluster[1]['secs_stamp'].values, \n",
    "                                            cluster[1]['Longitude'].values, \n",
    "                                            bounds_error=False,\n",
    "                                            fill_value=0.0)\n",
    "                ilat = flat(pdf['secs_stamp'].values)\n",
    "                ilon = flon(pdf['secs_stamp'].values)\n",
    "                pdf_lat += ilat\n",
    "                pdf_lon += ilon\n",
    "                # emergence --> sds :: plot --> Plot\n",
    "                pdf_plt[np.where(np.abs(ilat) > 0.0)] = cluster[1]['Plot'].unique()[0]\n",
    "        piccolo_dfs[k]['latitude'] = pdf_lat\n",
    "        piccolo_dfs[k]['longitude'] = pdf_lon\n",
    "        piccolo_dfs[k]['plot'] = pdf_plt\n",
    "        piccolo_dfs[k] = piccolo_dfs[k][piccolo_dfs[k]['plot'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the piccolo dataframes into files\n",
    "import shutil\n",
    "if os.path.exists(inter_dir):\n",
    "    shutil.rmtree(inter_dir)\n",
    "os.mkdir(inter_dir)\n",
    "for (i, k) in enumerate(piccolo_dfs):\n",
    "    cols1 = ['filename', 'device', 'secs_stamp', 'plot', 'latitude', 'longitude']\n",
    "    cols2 = ['direction', 'dark', 'replicate', 'integration_time', 'datetime', \n",
    "             'GPS_dt', 'date_stamp', 'time_stamp']\n",
    "    cols = piccolo_dfs[k].columns.values\n",
    "    waves = []\n",
    "    for c in cols:\n",
    "        if c not in cols1 and c not in cols2:\n",
    "            waves.append(c)\n",
    "    piccolo_dfs[k].to_csv(os.path.join(inter_dir, k + '_piccolo.csv'),\n",
    "                          columns=(cols1 + waves + cols2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
